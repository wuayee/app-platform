llms:
- 
   name: Qwen-72B
   model: Qwen
   scale: 72B
   type: "语言模型"
   orgnization: "Alibaba Cloud"
   description: "此模型为Qwen1.5-72B-Chat。Qwen1.5是Qwen2的 beta 版本，Qwen2是一种基于Transformer的解码器语言模型，经过大量数据预训练。"
   precision:
     default: fp16
     supports:
       - fp16
   gpu:
     min: 1
     max: 8
   npu:
     min: 4
     max: 8
   tokenSize:
     default: 8192
     min: 4096
     max: 8192
- 
   name: Qwen-14B-Chat
   model: Qwen
   scale: 14B
   type: "语言模型"
   orgnization: "Alibaba Cloud"
   description: "通义千问-14B（Qwen-14B）是阿里云研发的通义千问大模型系列的140亿参数规模的模型。Qwen-14B是基于Transformer的大语言模型, 在超大规模的预训练数据上进行训练得到。预训练数据类型多样，覆盖广泛，包括大量网络文本、专业书籍、代码等。"
   precision:
     default: fp16
     supports:
       - fp16
   gpu:
     min: 1
     max: 8
   npu:
     min: 1
     max: 8
   tokenSize:
     default: 8192
     min: 4096
     max: 8192
-
   name: chatglm3-6b
   model: chatglm3-6b
   scale:  6B
   type: "语言模型"
   orgnization: "智谱"
   description: "chatglm3-6b 是开源中英双语对话模型 ChatGLM-6B 的第三代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，ChatGLM2-6B 引入了更多新特性。"
   precision:
     default: fp16
     supports:
       - fp16
   gpu:
     min: 1
     max: 8
   npu:
     min: 1
     max: 8
-
   name: bge-large-zh
   model: BGE
   scale: 335M
   type: "Embedding"
   orgnization: "智源研究院"
   description: "bge-large-zh是由智源研究院研发的中文版文本表示模型，可将任意文本映射为低维稠密向量，以用于检索、分类、聚类或语义匹配等任务，并可支持为大模型调用外部知识。此为1.5版本，相似度分布更加合理。"
   precision:
     default: fp16
     supports:
       - fp16
   gpu:
     min: 1
     max: 1
   npu:
     min: 1
     max: 1
   tokenSize:
     default: 8192
     min: 4096
     max: 8192
