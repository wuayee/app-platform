# 聊天模型

## 简介

聊天模型是 FEL 的核心组件，其使用聊天消息作为输入，并返回聊天消息作为输出。为方便集成不同模型提供商（OpenAI、Qwen 等）提供的模型服务，FEL 抽象了一个标准接口来进行交互。

``` java
public interface ChatModel {
    /**
     * 调用聊天模型生成结果。
     *
     * @param prompt 表示提示词的 {@link Prompt}。
     * @param chatOption 表示聊天模型参数的 {@link ChatOption}。
     * @return 表示聊天模型生成结果的 {@link Choir}{@code <}{@link ChatMessage}{@code >}。
     */
    Choir<ChatMessage> generate(Prompt prompt, ChatOption chatOption);
}
```

聊天模型接收提示词以及一个可选参数，返回一个聊天消息流，根据可选参数中 `stream` 的值，返回一个或多个聊天消息。

## 示例

1. 在项目 pom.xml 加入以下依赖：

``` xml
<dependencies>
    <dependency>
        <groupId>modelengine.fit.starter</groupId>
        <artifactId>fit-starter</artifactId>
        <version>${fit.version}</version>
    </dependency>
    <dependency>
        <groupId>modelengine.fit.starter</groupId>
        <artifactId>fit-plugins-starter-web</artifactId>
        <version>${fit.version}</version>
    </dependency>
    <dependency>
        <groupId>modelengine.fit.plugin</groupId>
        <artifactId>fit-http-client-okhttp</artifactId>
        <version>${fit.version}</version>
    </dependency>
    <dependency>
        <groupId>modelengine.fit.jade.fel</groupId>
        <artifactId>fel-core</artifactId>
        <version>${fel.version}</version>
    </dependency>
    <dependency>
        <groupId>modelengine.fit.jade.community</groupId>
        <artifactId>fel-model-openai-plugin</artifactId>
        <version>${fel.version}</version>
    </dependency>
</dependencies>
```

2. 在 application.yml 配置文件中加入以下配置：

```yaml
fel:
  openai:
    api-base: '${api-base}'
    api-key: '${your-api-key}'
example:
  model: '${model-name}'
```

3. 添加如下代码：

``` java
@Component
public class ChatModelExampleController {
    private final ChatModel chatModel;
    @Value("${example.model}")
    private String modelName;

    public ChatModelExampleController(ChatModel chatModel) {
        this.chatModel = chatModel;
    }

    @GetMapping("/chat")
    public ChatMessage chat(@RequestParam("query") String query) {
        ChatOption option = ChatOption.custom().model(this.modelName).stream(false).build();
        return this.chatModel.generate(ChatMessages.from(new HumanMessage(query)), option).blockAll().get(0);
    }
}

```

4. 让模型返回流式输出：

``` java
@GetMapping("/chat-stream")
public Choir<ChatMessage> chatStream(@RequestParam("query") String query) {
    ChatOption option = ChatOption.custom().model(this.modelName).stream(true).build();
    return this.chatModel.generate(ChatMessages.from(new HumanMessage(query)), option);
}
```

至此，完成了聊天模型的接入，返回 Choir 对象，FIT 框架将自动使用 SSE 格式返回流式输出。

## 验证

1. 在浏览器栏输入：`http://localhost:8080/ai/example/chat?query=告诉我一个笑话`

   返回如下响应：

```json
{
  "content": "当然，接下来我将告诉你一个笑话：\n\n为什么袜子总是只丢一只？因为丢两只根本就不会发现。 \n\n希望这能给你带来一些欢笑！如果你还需要其他的笑话或者其他帮助，随时告诉我。",
  "toolCalls": []
}
```

2. 在浏览器栏输入：`http://localhost:8080/ai/example/chat-stream?query=告诉我一个笑话`

   返回如下响应：

```plaintext
data:{"content":"","toolCalls":[]}

data:{"content":"当然","toolCalls":[]}

data:{"content":"，","toolCalls":[]}

data:{"content":"接下来","toolCalls":[]}

data:{"content":"是一个","toolCalls":[]}

data:{"content":"轻松","toolCalls":[]}

data:{"content":"的","toolCalls":[]}

data:{"content":"笑话","toolCalls":[]}

data:{"content":"：\n\n","toolCalls":[]}

data:{"content":"为什么","toolCalls":[]}

data:{"content":"袜","toolCalls":[]}

...
```
